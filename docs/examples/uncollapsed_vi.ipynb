{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7d293e8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Sparse Stochastic Variational Inference\n",
    "\n",
    "In this notebook we demonstrate how to implement sparse variational Gaussian\n",
    "processes (SVGPs) of\n",
    "<strong data-cite=\"hensman2015gaussian\">Hensman et al. (2015)</strong>. In\n",
    "particular, this approximation framework provides a tractable option for working with\n",
    "non-conjugate Gaussian processes with more than ~5000 data points. However, for\n",
    "conjugate models of less than 5000 data points, we recommend using the marginal\n",
    "log-likelihood approach presented in the\n",
    "[regression notebook](https://docs.jaxgaussianprocesses.com/examples/regression/).\n",
    "Though we illustrate SVGPs here with a conjugate regression example, the same GPJax\n",
    "code works for general likelihoods, such as a Bernoulli for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38db9007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Float64 for more stable matrix inversions.\n",
    "from jax import config\n",
    "\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "from jax import jit\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "from jaxtyping import install_import_hook\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import optax as ox\n",
    "import tensorflow_probability.substrates.jax as tfp\n",
    "\n",
    "with install_import_hook(\"gpjax\", \"beartype.beartype\"):\n",
    "    import gpjax as gpx\n",
    "    import gpjax.kernels as jk\n",
    "\n",
    "key = jr.PRNGKey(123)\n",
    "tfb = tfp.bijectors\n",
    "plt.style.use(\n",
    "    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n",
    ")\n",
    "cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e145ef57",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "With the necessary modules imported, we simulate a dataset\n",
    "$\\mathcal{D} = (\\boldsymbol{x}, \\boldsymbol{y}) = \\{(x_i, y_i)\\}_{i=1}^{5000}$\n",
    "with inputs $\\boldsymbol{x}$ sampled uniformly on $(-5, 5)$ and corresponding binary outputs\n",
    "\n",
    "$$\\boldsymbol{y} \\sim \\mathcal{N} \\left(\\sin(4 * \\boldsymbol{x}) + \\sin(2 * \\boldsymbol{x}), \\textbf{I} * (0.2)^{2} \\right).$$\n",
    "\n",
    "We store our data $\\mathcal{D}$ as a GPJax `Dataset` and create test inputs for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb2fec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50000\n",
    "noise = 0.2\n",
    "\n",
    "key, subkey = jr.split(key)\n",
    "x = jr.uniform(key=key, minval=-5.0, maxval=5.0, shape=(n,)).reshape(-1, 1)\n",
    "f = lambda x: jnp.sin(4 * x) + jnp.cos(2 * x)\n",
    "signal = f(x)\n",
    "y = signal + jr.normal(subkey, shape=signal.shape) * noise\n",
    "D = gpx.Dataset(X=x, y=y)\n",
    "\n",
    "xtest = jnp.linspace(-5.5, 5.5, 500).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84132669",
   "metadata": {},
   "source": [
    "## Sparse GPs via inducing inputs\n",
    "\n",
    "Despite their endowment with elegant theoretical properties, GPs are burdened with\n",
    "prohibitive $\\mathcal{O}(n^3)$ inference and $\\mathcal{O}(n^2)$ memory costs in the\n",
    "number of data points $n$ due to the necessity of computing inverses and determinants\n",
    "of the kernel Gram matrix $\\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}}$ during inference\n",
    "and hyperparameter learning.\n",
    "Sparse GPs seek to resolve tractability through low-rank approximations.\n",
    "\n",
    "Their name originates with the idea of using subsets of the data to approximate the\n",
    "kernel matrix, with _sparseness_ occurring through the selection of the data points.\n",
    "Given inputs $\\boldsymbol{x}$ and outputs $\\boldsymbol{y}$ the task was to select an\n",
    "$m<n$ lower-dimensional dataset $(\\boldsymbol{z},\\boldsymbol{\\tilde{y}}) \\subset (\\boldsymbol{x}, \\boldsymbol{y})$\n",
    "to train a Gaussian process on instead.\n",
    "By generalising the set of selected points $\\boldsymbol{z}$, known as\n",
    "_inducing inputs_, to remove the restriction of being part of the dataset,\n",
    "we can arrive at a flexible low-rank approximation framework of the model using\n",
    "functions of $\\mathbf{K}_{\\boldsymbol{z}\\boldsymbol{z}}$ to replace the true\n",
    "covariance matrix $\\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}}$ at significantly\n",
    "lower costs. For example, <strong data-cite=\"quinonero-candela2005gaussian\"></strong>\n",
    "review many popular approximation schemes in this vein. However, because the model\n",
    "and the approximation are intertwined, assigning performance and faults to one or the\n",
    "other becomes tricky.\n",
    "\n",
    "On the other hand, sparse variational Gaussian processes (SVGPs)\n",
    "[approximate the posterior, not the model](https://www.secondmind.ai/labs/sparse-gps-approximate-the-posterior-not-the-model/).\n",
    "These provide a low-rank approximation scheme via variational inference. Here we\n",
    "posit a family of densities parameterised by “variational parameters”.\n",
    "We then seek to find the closest family member to the posterior by minimising the\n",
    "Kullback-Leibler divergence over the variational parameters.\n",
    "The fitted variational density then serves as a proxy for the exact posterior.\n",
    "This procedure makes variational methods efficiently solvable via off-the-shelf\n",
    "optimisation techniques whilst retaining the true-underlying model.\n",
    "Furthermore, SVGPs offer further cost reductions with mini-batch stochastic gradient\n",
    "descent  <strong data-cite=\"hensman2013gaussian\"></strong> and address non-conjugacy\n",
    "<strong data-cite=\"hensman2015gaussian\"></strong>.\n",
    "We show a cost comparison between the approaches below, where $b$ is the mini-batch\n",
    "size.\n",
    "\n",
    "|    | GPs | sparse GPs | SVGP |\n",
    "| -- | -- | -- | -- |\n",
    "| Inference cost | $\\mathcal{O}(n^3)$ | $\\mathcal{O}(n m^2)$ | $\\mathcal{O}(b m^2 + m^3)$  |\n",
    "| Memory cost    | $\\mathcal{O}(n^2)$ | $\\mathcal{O}(n m)$ | $\\mathcal{O}(b m + m^2)$ |\n",
    "\n",
    "To apply SVGP inference to our dataset, we begin by initialising $m = 50$ equally\n",
    "spaced inducing inputs $\\boldsymbol{z}$ across our observed data's support. These\n",
    "are depicted below via horizontal black lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e5522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = jnp.linspace(-5.0, 5.0, 50).reshape(-1, 1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.vlines(\n",
    "    z,\n",
    "    ymin=y.min(),\n",
    "    ymax=y.max(),\n",
    "    alpha=0.3,\n",
    "    linewidth=1,\n",
    "    label=\"Inducing point\",\n",
    "    color=cols[2],\n",
    ")\n",
    "ax.scatter(x, y, alpha=0.2, color=cols[0], label=\"Observations\")\n",
    "ax.plot(xtest, f(xtest), color=cols[1], label=\"Latent function\")\n",
    "ax.legend()\n",
    "ax.set(xlabel=r\"$x$\", ylabel=r\"$f(x)$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8cf101",
   "metadata": {},
   "source": [
    "The inducing inputs will summarise our dataset, and since they are treated as\n",
    "variational parameters, their locations will be optimised. The next step to SVGP is\n",
    "to define a variational family."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d399f2",
   "metadata": {},
   "source": [
    "## Defining the variational process\n",
    "\n",
    "We begin by considering the form of the posterior distribution for all function\n",
    "values $f(\\cdot)$\n",
    "\n",
    "\\begin{align}\n",
    "p(f(\\cdot) | \\mathcal{D}) = \\int p(f(\\cdot)|f(\\boldsymbol{x})) p(f(\\boldsymbol{x})|\\mathcal{D}) \\text{d}f(\\boldsymbol{x}). \\qquad (\\dagger)\n",
    "\\end{align}\n",
    "\n",
    "To arrive at an approximation framework, we assume some redundancy in the data.\n",
    "Instead of predicting $f(\\cdot)$ with function values at the datapoints\n",
    "$f(\\boldsymbol{x})$, we assume this can be achieved with only function values at\n",
    "$m$ inducing inputs $\\boldsymbol{z}$\n",
    "\n",
    "$$ p(f(\\cdot) | \\mathcal{D}) \\approx \\int p(f(\\cdot)|f(\\boldsymbol{z})) p(f(\\boldsymbol{z})|\\mathcal{D}) \\text{d}f(\\boldsymbol{z}). \\qquad (\\star) $$\n",
    "\n",
    "This lower dimensional integral results in computational savings in the model's\n",
    "predictive component from $p(f(\\cdot)|f(\\boldsymbol{x}))$ to\n",
    "$p(f(\\cdot)|f(\\boldsymbol{z}))$ where inverting\n",
    "$\\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}}$ is replaced with inverting\n",
    "$\\mathbf{K}_{\\boldsymbol{z}\\boldsymbol{z}}$.\n",
    "However, since we did not observe our data $\\mathcal{D}$ at $\\boldsymbol{z}$ we ask,\n",
    "what exactly is the posterior $p(f(\\boldsymbol{z})|\\mathcal{D})$?\n",
    "\n",
    "Notice this is simply obtained by substituting $\\boldsymbol{z}$ into $(\\dagger)$,\n",
    "but we arrive back at square one with computing the expensive integral. To side-step\n",
    "this, we consider replacing $p(f(\\boldsymbol{z})|\\mathcal{D})$ in $(\\star)$ with a\n",
    "cheap-to-compute approximate distribution $q(f(\\boldsymbol{z}))$\n",
    "\n",
    "  $$ q(f(\\cdot)) = \\int p(f(\\cdot)|f(\\boldsymbol{z})) q(f(\\boldsymbol{z})) \\text{d}f(\\boldsymbol{z}). \\qquad (\\times) $$\n",
    "\n",
    "To measure the quality of the approximation, we consider the Kullback-Leibler\n",
    "divergence $\\operatorname{KL}(\\cdot || \\cdot)$ from our approximate process\n",
    "$q(f(\\cdot))$ to the true process $p(f(\\cdot)|\\mathcal{D})$. By parametrising\n",
    "$q(f(\\boldsymbol{z}))$ over a variational family of distributions, we can optimise\n",
    "Kullback-Leibler divergence with respect to the variational parameters. Moreover,\n",
    "since inducing input locations $\\boldsymbol{z}$ augment the model, they themselves\n",
    "can be treated as variational parameters without altering the true underlying model\n",
    "$p(f(\\boldsymbol{z})|\\mathcal{D})$. This is exactly what gives SVGPs great\n",
    "flexibility whilst retaining robustness to overfitting.\n",
    "\n",
    "It is popular to elect a Gaussian variational distribution\n",
    "$q(f(\\boldsymbol{z})) = \\mathcal{N}(f(\\boldsymbol{z}); \\mathbf{m}, \\mathbf{S})$\n",
    "with parameters $\\{\\boldsymbol{z}, \\mathbf{m}, \\mathbf{S}\\}$, since conjugacy is\n",
    "provided between $q(f(\\boldsymbol{z}))$ and $p(f(\\cdot)|f(\\boldsymbol{z}))$ so that\n",
    "the resulting variational process $q(f(\\cdot))$ is a GP. We can implement this in\n",
    "GPJax by the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22395646",
   "metadata": {},
   "outputs": [],
   "source": [
    "meanf = gpx.mean_functions.Zero()\n",
    "likelihood = gpx.likelihoods.Gaussian(num_datapoints=n)\n",
    "prior = gpx.gps.Prior(mean_function=meanf, kernel=jk.RBF())\n",
    "p = prior * likelihood\n",
    "q = gpx.variational_families.VariationalGaussian(posterior=p, inducing_inputs=z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec38370",
   "metadata": {},
   "source": [
    "Here, the variational process $q(\\cdot)$ depends on the prior through\n",
    "$p(f(\\cdot)|f(\\boldsymbol{z}))$ in $(\\times)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0176ff",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "### Evidence lower bound\n",
    "\n",
    "With our model defined, we seek to infer the optimal inducing inputs\n",
    "$\\boldsymbol{z}$, variational mean $\\mathbf{m}$ and covariance\n",
    "$\\mathbf{S}$ that define our approximate posterior. To achieve this, we maximise the\n",
    "evidence lower bound (ELBO) with respect to\n",
    "$\\{\\boldsymbol{z}, \\mathbf{m}, \\mathbf{S} \\}$, a proxy for minimising the\n",
    "Kullback-Leibler divergence. Moreover, as hinted by its name, the ELBO is a lower\n",
    "bound to the marginal log-likelihood, providing a tractable objective to optimise the\n",
    "model's hyperparameters akin to the conjugate setting. For further details on this,\n",
    "see Sections 3.1 and 4.1 of the excellent review paper\n",
    "<strong data-cite=\"leibfried2020tutorial\"></strong>.\n",
    "\n",
    "Since Optax's optimisers work to minimise functions, to maximise the ELBO we return\n",
    "its negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164d9de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_elbo = gpx.objectives.ELBO(negative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0287cb5",
   "metadata": {},
   "source": [
    "For researchers, GPJax has the capacity to print the bibtex citation for objects such\n",
    "as the ELBO through the `cite()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2d215c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gpx.cite(negative_elbo))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b56eec",
   "metadata": {},
   "source": [
    "JIT-compiling expensive-to-compute functions such as the ELBO is\n",
    "advisable. This can be achieved by wrapping the function in `jax.jit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef365994",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_elbo = jit(negative_elbo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de18c1e7",
   "metadata": {},
   "source": [
    "### Mini-batching\n",
    "\n",
    "Despite introducing inducing inputs into our model, inference can still be\n",
    "intractable with large datasets. To circumvent this, optimisation can be done using\n",
    "stochastic mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec3f934",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "schedule = ox.warmup_cosine_decay_schedule(\n",
    "    init_value=0.0,\n",
    "    peak_value=0.01,\n",
    "    warmup_steps=75,\n",
    "    decay_steps=1500,\n",
    "    end_value=0.001,\n",
    ")\n",
    "\n",
    "opt_posterior, history = gpx.fit(\n",
    "    model=q,\n",
    "    objective=negative_elbo,\n",
    "    train_data=D,\n",
    "    optim=ox.adam(learning_rate=schedule),\n",
    "    num_iters=3000,\n",
    "    key=jr.PRNGKey(42),\n",
    "    batch_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834ebd5f",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "\n",
    "With optimisation complete, we can use our inferred parameter set to make\n",
    "predictions at novel inputs akin\n",
    "to all other models within GPJax on our variational process object $q(\\cdot)$ (for\n",
    "example, see the\n",
    "[regression notebook](https://docs.jaxgaussianprocesses.com/examples/regression/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0d8401",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dist = opt_posterior(xtest)\n",
    "predictive_dist = opt_posterior.posterior.likelihood(latent_dist)\n",
    "\n",
    "meanf = predictive_dist.mean()\n",
    "sigma = predictive_dist.stddev()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y, alpha=0.15, label=\"Training Data\", color=cols[0])\n",
    "ax.plot(xtest, meanf, label=\"Posterior mean\", color=cols[1])\n",
    "ax.fill_between(\n",
    "    xtest.flatten(),\n",
    "    meanf - 2 * sigma,\n",
    "    meanf + 2 * sigma,\n",
    "    alpha=0.3,\n",
    "    color=cols[1],\n",
    "    label=\"Two sigma\",\n",
    ")\n",
    "ax.vlines(\n",
    "    opt_posterior.inducing_inputs,\n",
    "    ymin=y.min(),\n",
    "    ymax=y.max(),\n",
    "    alpha=0.3,\n",
    "    linewidth=1,\n",
    "    label=\"Inducing point\",\n",
    "    color=cols[2],\n",
    ")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b12de0b",
   "metadata": {},
   "source": [
    "## Custom transformations\n",
    "\n",
    "To train a covariance matrix, GPJax uses `tfb.FillScaleTriL` transformation by\n",
    "default. `tfb.FillScaleTriL` fills a 1d vector into a lower triangular matrix and\n",
    "then applies `Softplus` transformation on the diagonal to satisfy the necessary\n",
    "conditions for a valid Cholesky matrix. Users can change this default transformation\n",
    "with another valid transformation of their choice. For example, `Square`\n",
    "transformation on the diagonal can also serve the purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1871f717",
   "metadata": {},
   "outputs": [],
   "source": [
    "triangular_transform = tfb.FillScaleTriL(\n",
    "    diag_bijector=tfb.Square(), diag_shift=jnp.array(q.jitter)\n",
    ")\n",
    "reparameterised_q = q.replace_bijector(variational_root_covariance=triangular_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae78589f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "opt_rep, history = gpx.fit(\n",
    "    model=reparameterised_q,\n",
    "    objective=negative_elbo,\n",
    "    train_data=D,\n",
    "    optim=ox.adam(learning_rate=0.01),\n",
    "    num_iters=3000,\n",
    "    key=jr.PRNGKey(42),\n",
    "    batch_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e57303c",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dist = opt_rep(xtest)\n",
    "predictive_dist = opt_rep.posterior.likelihood(latent_dist)\n",
    "\n",
    "meanf = predictive_dist.mean()\n",
    "sigma = predictive_dist.stddev()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y, alpha=0.15, label=\"Training Data\", color=cols[0])\n",
    "ax.plot(xtest, meanf, label=\"Posterior mean\", color=cols[1])\n",
    "ax.fill_between(\n",
    "    xtest.flatten(),\n",
    "    meanf - 2 * sigma,\n",
    "    meanf + 2 * sigma,\n",
    "    alpha=0.3,\n",
    "    color=cols[1],\n",
    "    label=\"Two sigma\",\n",
    ")\n",
    "ax.vlines(\n",
    "    opt_rep.inducing_inputs,\n",
    "    ymin=y.min(),\n",
    "    ymax=y.max(),\n",
    "    alpha=0.3,\n",
    "    linewidth=1,\n",
    "    label=\"Inducing point\",\n",
    "    color=cols[2],\n",
    ")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad399a78",
   "metadata": {},
   "source": [
    "We can see that `Square` transformation is able to get relatively better fit\n",
    "compared to `Softplus` with the same number of iterations, but `Softplus` is\n",
    "recommended over `Square` for stability of optimisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1450d99",
   "metadata": {},
   "source": [
    "## System configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e224a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -n -u -v -iv -w -a 'Thomas Pinder, Daniel Dodd & Zeel B Patel'"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "custom_cell_magics": "kql",
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "gpjax_beartype",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
