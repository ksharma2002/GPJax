{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b84b95e2",
   "metadata": {},
   "source": [
    "# Sparse Gaussian Process Regression\n",
    "\n",
    "In this notebook we consider sparse Gaussian process regression (SGPR)\n",
    "<strong data-cite=\"titsias2009\">Titsias (2009)</strong>. This is a solution for\n",
    "medium to large-scale conjugate regression problems.\n",
    "In order to arrive at a computationally tractable method, the approximate posterior\n",
    "is parameterized via a set of $m$ pseudo-points $\\boldsymbol{z}$. Critically, the\n",
    "approach leads to $\\mathcal{O}(nm^2)$ complexity for approximate maximum likelihood\n",
    "learning and $O(m^2)$ per test point for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9bc96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Float64 for more stable matrix inversions.\n",
    "from jax import config\n",
    "\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "from jax import jit\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "from jaxtyping import install_import_hook\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import optax as ox\n",
    "from docs.examples.utils import clean_legend\n",
    "\n",
    "with install_import_hook(\"gpjax\", \"beartype.beartype\"):\n",
    "    import gpjax as gpx\n",
    "\n",
    "key = jr.PRNGKey(123)\n",
    "plt.style.use(\n",
    "    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n",
    ")\n",
    "cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b7765a",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "With the necessary modules imported, we simulate a dataset\n",
    "$\\mathcal{D} = (\\boldsymbol{x}, \\boldsymbol{y}) = \\{(x_i, y_i)\\}_{i=1}^{500}$\n",
    "with inputs $\\boldsymbol{x}$ sampled uniformly on $(-3., 3)$ and corresponding\n",
    "independent noisy outputs\n",
    "\n",
    "$$\\boldsymbol{y} \\sim \\mathcal{N} \\left(\\sin(7\\boldsymbol{x}) + x \\cos(2 \\boldsymbol{x}), \\textbf{I} * 0.5^2 \\right).$$\n",
    "\n",
    "We store our data $\\mathcal{D}$ as a GPJax `Dataset` and create test inputs and\n",
    "labels for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15b1608",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2500\n",
    "noise = 0.5\n",
    "\n",
    "key, subkey = jr.split(key)\n",
    "x = jr.uniform(key=key, minval=-3.0, maxval=3.0, shape=(n,)).reshape(-1, 1)\n",
    "f = lambda x: jnp.sin(2 * x) + x * jnp.cos(5 * x)\n",
    "signal = f(x)\n",
    "y = signal + jr.normal(subkey, shape=signal.shape) * noise\n",
    "\n",
    "D = gpx.Dataset(X=x, y=y)\n",
    "\n",
    "xtest = jnp.linspace(-3.1, 3.1, 500).reshape(-1, 1)\n",
    "ytest = f(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4b6e06",
   "metadata": {},
   "source": [
    "To better understand what we have simulated, we plot both the underlying latent\n",
    "function and the observed data that is subject to Gaussian noise. We also plot an\n",
    "initial set of inducing points over the space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e20d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inducing = 50\n",
    "z = jnp.linspace(-3.0, 3.0, n_inducing).reshape(-1, 1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y, alpha=0.25, label=\"Observations\", color=cols[0])\n",
    "ax.plot(xtest, ytest, label=\"Latent function\", linewidth=2, color=cols[1])\n",
    "ax.vlines(\n",
    "    x=z,\n",
    "    ymin=y.min(),\n",
    "    ymax=y.max(),\n",
    "    alpha=0.3,\n",
    "    linewidth=0.5,\n",
    "    label=\"Inducing point\",\n",
    "    color=cols[2],\n",
    ")\n",
    "ax.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af946ecf",
   "metadata": {},
   "source": [
    "Next we define the true posterior model for the data - note that whilst we can define\n",
    "this, it is intractable to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbcc213",
   "metadata": {},
   "outputs": [],
   "source": [
    "meanf = gpx.mean_functions.Constant()\n",
    "kernel = gpx.kernels.RBF()\n",
    "likelihood = gpx.likelihoods.Gaussian(num_datapoints=D.n)\n",
    "prior = gpx.gps.Prior(mean_function=meanf, kernel=kernel)\n",
    "posterior = prior * likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ba0539",
   "metadata": {},
   "source": [
    "We now define the SGPR model through `CollapsedVariationalGaussian`. Through a\n",
    "set of inducing points $\\boldsymbol{z}$ this object builds an approximation to the\n",
    "true posterior distribution. Consequently, we pass the true posterior and initial\n",
    "inducing points into the constructor as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a459913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = gpx.variational_families.CollapsedVariationalGaussian(\n",
    "    posterior=posterior, inducing_inputs=z\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fb0e13",
   "metadata": {},
   "source": [
    "We define our variational inference algorithm through `CollapsedVI`. This defines\n",
    "the collapsed variational free energy bound considered in\n",
    "<strong data-cite=\"titsias2009\">Titsias (2009)</strong>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6476af",
   "metadata": {},
   "outputs": [],
   "source": [
    "elbo = gpx.objectives.CollapsedELBO(negative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28702973",
   "metadata": {},
   "source": [
    "For researchers, GPJax has the capacity to print the bibtex citation for objects such\n",
    "as the ELBO through the `cite()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc2975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gpx.cite(elbo))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48f8256",
   "metadata": {},
   "source": [
    "JIT-compiling expensive-to-compute functions such as the ELBO is\n",
    "advisable. This can be achieved by wrapping the function in `jax.jit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060c4142",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "elbo = jit(elbo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465806bf",
   "metadata": {},
   "source": [
    "We now train our model akin to a Gaussian process regression model via the `fit`\n",
    "abstraction. Unlike the regression example given in the\n",
    "[conjugate regression notebook](https://docs.jaxgaussianprocesses.com/examples/regression/),\n",
    "the inducing locations that induce our variational posterior distribution are now\n",
    "part of the model's parameters. Using a gradient-based optimiser, we can then\n",
    "_optimise_ their location such that the evidence lower bound is maximised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bd9987",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_posterior, history = gpx.fit(\n",
    "    model=q,\n",
    "    objective=elbo,\n",
    "    train_data=D,\n",
    "    optim=ox.adamw(learning_rate=1e-2),\n",
    "    num_iters=500,\n",
    "    key=key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea7bb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(history, color=cols[1])\n",
    "ax.set(xlabel=\"Training iterate\", ylabel=\"ELBO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1f027e",
   "metadata": {},
   "source": [
    "We show predictions of our model with the learned inducing points overlaid in grey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11b63de",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dist = opt_posterior(xtest, train_data=D)\n",
    "predictive_dist = opt_posterior.posterior.likelihood(latent_dist)\n",
    "\n",
    "inducing_points = opt_posterior.inducing_inputs\n",
    "\n",
    "samples = latent_dist.sample(seed=key, sample_shape=(20,))\n",
    "\n",
    "predictive_mean = predictive_dist.mean()\n",
    "predictive_std = predictive_dist.stddev()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x, y, \"x\", label=\"Observations\", color=cols[0], alpha=0.1)\n",
    "ax.plot(\n",
    "    xtest,\n",
    "    ytest,\n",
    "    label=\"Latent function\",\n",
    "    color=cols[1],\n",
    "    linestyle=\"-\",\n",
    "    linewidth=1,\n",
    ")\n",
    "ax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1])\n",
    "\n",
    "ax.fill_between(\n",
    "    xtest.squeeze(),\n",
    "    predictive_mean - 2 * predictive_std,\n",
    "    predictive_mean + 2 * predictive_std,\n",
    "    alpha=0.2,\n",
    "    color=cols[1],\n",
    "    label=\"Two sigma\",\n",
    ")\n",
    "ax.plot(\n",
    "    xtest,\n",
    "    predictive_mean - 2 * predictive_std,\n",
    "    color=cols[1],\n",
    "    linestyle=\"--\",\n",
    "    linewidth=0.5,\n",
    ")\n",
    "ax.plot(\n",
    "    xtest,\n",
    "    predictive_mean + 2 * predictive_std,\n",
    "    color=cols[1],\n",
    "    linestyle=\"--\",\n",
    "    linewidth=0.5,\n",
    ")\n",
    "\n",
    "\n",
    "ax.vlines(\n",
    "    x=inducing_points,\n",
    "    ymin=ytest.min(),\n",
    "    ymax=ytest.max(),\n",
    "    alpha=0.3,\n",
    "    linewidth=0.5,\n",
    "    label=\"Inducing point\",\n",
    "    color=cols[2],\n",
    ")\n",
    "ax.legend()\n",
    "ax.set(xlabel=r\"$x$\", ylabel=r\"$f(x)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fdc56a",
   "metadata": {},
   "source": [
    "## Runtime comparison\n",
    "\n",
    "Given the size of the data being considered here, inference in a GP with a full-rank\n",
    "covariance matrix is possible, albeit quite slow. We can therefore compare the\n",
    "speedup that we get from using the above sparse approximation with corresponding\n",
    "bound on the marginal log-likelihood against the marginal log-likelihood in the\n",
    "full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a245fd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_rank_model = gpx.gps.Prior(\n",
    "    mean_function=gpx.mean_functions.Zero(), kernel=gpx.kernels.RBF()\n",
    ") * gpx.likelihoods.Gaussian(num_datapoints=D.n)\n",
    "negative_mll = jit(gpx.objectives.ConjugateMLL(negative=True).step)\n",
    "%timeit negative_mll(full_rank_model, D).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04296a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_elbo = jit(gpx.objectives.CollapsedELBO(negative=True).step)\n",
    "%timeit negative_elbo(q, D).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d11d613",
   "metadata": {},
   "source": [
    "As we can see, the sparse approximation given here is around 50 times faster when\n",
    "compared against a full-rank model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426f893b",
   "metadata": {},
   "source": [
    "## System configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2a4938",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -n -u -v -iv -w -a 'Daniel Dodd'"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "custom_cell_magics": "kql"
  },
  "kernelspec": {
   "display_name": "gpjax_beartype",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
