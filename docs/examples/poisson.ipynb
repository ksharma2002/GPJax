{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0329b27",
   "metadata": {},
   "source": [
    "# Count data regression\n",
    "\n",
    "In this notebook we demonstrate how to perform inference for Gaussian process models\n",
    "with non-Gaussian likelihoods via Markov chain Monte\n",
    "Carlo (MCMC). We focus on a count data regression task here and use\n",
    "[BlackJax](https://github.com/blackjax-devs/blackjax/) for sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f214051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import blackjax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import jax.tree_util as jtu\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_probability.substrates.jax as tfp\n",
    "from jax import config\n",
    "from jaxtyping import install_import_hook\n",
    "\n",
    "with install_import_hook(\"gpjax\", \"beartype.beartype\"):\n",
    "    import gpjax as gpx\n",
    "\n",
    "# Enable Float64 for more stable matrix inversions.\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "tfd = tfp.distributions\n",
    "key = jr.PRNGKey(123)\n",
    "plt.style.use(\n",
    "    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n",
    ")\n",
    "cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac3b19c",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "For count data regression, the Poisson distribution is a natural choice for the likelihood function. The probability mass function of the Poisson distribution is given by\n",
    "\n",
    "$$ p(y \\,|\\, \\lambda) = \\frac{\\lambda^{y} e^{-\\lambda}}{y!},$$\n",
    "\n",
    "where $y$ is the count and the parameter $\\lambda \\in \\mathbb{R}_{>0}$ is the rate of the Poisson distribution.\n",
    "\n",
    "We than set $\\lambda = \\exp(f)$ where $f$ is the latent Gaussian process. The exponential function is the _link function_ for the Poisson distribution: it maps the output of a GP to the positive real line, which is suitable for modeling count data.\n",
    "\n",
    "We simulate a dataset $\\mathcal{D} = \\{(\\mathbf{X}, \\mathbf{y})\\}$ with inputs $\\mathbf{X} \\in \\mathbb{R}^d$ and corresponding count outputs $\\mathbf{y}$.\n",
    "We store our data $\\mathcal{D}$ as a GPJax `Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce94cdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = jr.split(key)\n",
    "n = 50\n",
    "x = jr.uniform(key, shape=(n, 1), minval=-2.0, maxval=2.0)\n",
    "f = lambda x: 2.0 * jnp.sin(3 * x) + 0.5 * x  # latent function\n",
    "y = jr.poisson(key, jnp.exp(f(x)))\n",
    "\n",
    "D = gpx.Dataset(X=x, y=y)\n",
    "\n",
    "xtest = jnp.linspace(-2.0, 2.0, 500).reshape(-1, 1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y, \"o\", label=\"Observations\", color=cols[1])\n",
    "ax.plot(xtest, jnp.exp(f(xtest)), label=r\"Rate $\\lambda$\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95c4f3c",
   "metadata": {},
   "source": [
    "## Gaussian Process definition\n",
    "\n",
    "We begin by defining a Gaussian process prior with a radial basis function (RBF)\n",
    "kernel, chosen for the purpose of exposition. We adopt the Poisson likelihood available in GPJax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f37053",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = gpx.kernels.RBF()\n",
    "meanf = gpx.mean_functions.Constant()\n",
    "prior = gpx.gps.Prior(mean_function=meanf, kernel=kernel)\n",
    "likelihood = gpx.likelihoods.Poisson(num_datapoints=D.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf7e127",
   "metadata": {},
   "source": [
    "We construct the posterior through the product of our prior and likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2f4d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = prior * likelihood\n",
    "print(type(posterior))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0be3a8d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Whilst the latent function is Gaussian, the posterior distribution is non-Gaussian\n",
    "since our generative model first samples the latent GP and propagates these samples\n",
    "through the likelihood function's inverse link function. This step prevents us from\n",
    "being able to analytically integrate the latent function's values out of our\n",
    "posterior, and we must instead adopt alternative inference techniques. Here, we show\n",
    "how to use MCMC methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9628726c",
   "metadata": {},
   "source": [
    "## MCMC inference\n",
    "\n",
    "An MCMC sampler works by starting at an initial position and\n",
    "drawing a sample from a cheap-to-simulate distribution known as the _proposal_. The\n",
    "next step is to determine whether this sample could be considered a draw from the\n",
    "posterior. We accomplish this using an _acceptance probability_ determined via the\n",
    "sampler's _transition kernel_ which depends on the current position and the\n",
    "unnormalised target posterior distribution. If the new sample is more _likely_, we\n",
    "accept it; otherwise, we reject it and stay in our current position. Repeating these\n",
    "steps results in a Markov chain (a random sequence that depends only on the last\n",
    "state) whose stationary distribution (the long-run empirical distribution of the\n",
    "states visited) is the posterior. For a gentle introduction, see the first chapter\n",
    "of [A Handbook of Markov Chain Monte Carlo](https://www.mcmchandbook.net/HandbookChapter1.pdf).\n",
    "\n",
    "### MCMC through BlackJax\n",
    "\n",
    "Rather than implementing a suite of MCMC samplers, GPJax relies on MCMC-specific\n",
    "libraries for sampling functionality. We focus on\n",
    "[BlackJax](https://github.com/blackjax-devs/blackjax/) in this notebook, which we\n",
    "recommend adopting for general applications.\n",
    "\n",
    "We begin by generating _sensible_ initial positions for our sampler before defining\n",
    "an inference loop and sampling 200 values from our Markov chain. In practice,\n",
    "drawing more samples will be necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7d3e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from BlackJax's introduction notebook.\n",
    "num_adapt = 100\n",
    "num_samples = 200\n",
    "\n",
    "lpd = jax.jit(gpx.objectives.LogPosteriorDensity(negative=False))\n",
    "unconstrained_lpd = jax.jit(lambda tree: lpd(tree.constrain(), D))\n",
    "\n",
    "adapt = blackjax.window_adaptation(\n",
    "    blackjax.nuts, unconstrained_lpd, num_adapt, target_acceptance_rate=0.65\n",
    ")\n",
    "\n",
    "# Initialise the chain\n",
    "last_state, kernel, _ = adapt.run(key, posterior.unconstrain())\n",
    "\n",
    "\n",
    "def inference_loop(rng_key, kernel, initial_state, num_samples):\n",
    "    def one_step(state, rng_key):\n",
    "        state, info = kernel(rng_key, state)\n",
    "        return state, (state, info)\n",
    "\n",
    "    keys = jax.random.split(rng_key, num_samples)\n",
    "    _, (states, infos) = jax.lax.scan(one_step, initial_state, keys)\n",
    "\n",
    "    return states, infos\n",
    "\n",
    "\n",
    "# Sample from the posterior distribution\n",
    "states, infos = inference_loop(key, kernel, last_state, num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfdc09a",
   "metadata": {},
   "source": [
    "### Sampler efficiency\n",
    "\n",
    "BlackJax gives us easy access to our sampler's efficiency through metrics such as the\n",
    "sampler's _acceptance probability_ (the number of times that our chain accepted a\n",
    "proposed sample, divided by the total number of steps run by the chain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656b07b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptance_rate = jnp.mean(infos.acceptance_probability)\n",
    "print(f\"Acceptance rate: {acceptance_rate:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991f2e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(10, 3))\n",
    "ax0.plot(states.position.constrain().prior.kernel.variance)\n",
    "ax1.plot(states.position.constrain().prior.kernel.lengthscale)\n",
    "ax2.plot(states.position.constrain().prior.mean_function.constant)\n",
    "ax0.set_title(\"Kernel variance\")\n",
    "ax1.set_title(\"Kernel lengthscale\")\n",
    "ax2.set_title(\"Mean function constant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c76bea4",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "Having obtained samples from the posterior, we draw ten instances from our model's\n",
    "predictive distribution per MCMC sample. Using these draws, we will be able to\n",
    "compute credible values and expected values under our posterior distribution.\n",
    "\n",
    "An ideal Markov chain would have samples completely uncorrelated with their\n",
    "neighbours after a single lag. However, in practice, correlations often exist\n",
    "within our chain's sample set. A commonly used technique to try and reduce this\n",
    "correlation is _thinning_ whereby we select every $n$-th sample where $n$ is the\n",
    "minimum lag length at which we believe the samples are uncorrelated. Although further\n",
    "analysis of the chain's autocorrelation is required to find appropriate thinning\n",
    "factors, we employ a thin factor of 10 for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a9c30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "thin_factor = 10\n",
    "samples = []\n",
    "\n",
    "for i in range(num_adapt, num_samples + num_adapt, thin_factor):\n",
    "    sample = jtu.tree_map(lambda samples: samples[i], states.position)\n",
    "    sample = sample.constrain()\n",
    "    latent_dist = sample.predict(xtest, train_data=D)\n",
    "    predictive_dist = sample.likelihood(latent_dist)\n",
    "    samples.append(predictive_dist.sample(seed=key, sample_shape=(10,)))\n",
    "\n",
    "samples = jnp.vstack(samples)\n",
    "\n",
    "lower_ci, upper_ci = jnp.percentile(samples, jnp.array([2.5, 97.5]), axis=0)\n",
    "expected_val = jnp.mean(samples, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a76850c",
   "metadata": {},
   "source": [
    "\n",
    "Finally, we end this tutorial by plotting the predictions obtained from our model\n",
    "against the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10986ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(\n",
    "    x, y, \"o\", markersize=5, color=cols[1], label=\"Observations\", zorder=2, alpha=0.7\n",
    ")\n",
    "ax.plot(\n",
    "    xtest, expected_val, linewidth=2, color=cols[0], label=\"Predicted mean\", zorder=1\n",
    ")\n",
    "ax.fill_between(\n",
    "    xtest.flatten(),\n",
    "    lower_ci.flatten(),\n",
    "    upper_ci.flatten(),\n",
    "    alpha=0.2,\n",
    "    color=cols[0],\n",
    "    label=\"95% CI\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77639a46",
   "metadata": {},
   "source": [
    "## System configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8d0d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w -a \"Francesco Zanetta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8ed917",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "custom_cell_magics": "kql"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
