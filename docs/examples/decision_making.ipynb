{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64e91cda",
   "metadata": {},
   "source": [
    "# Introduction to Decision Making with GPJax\n",
    "\n",
    "In this notebook we provide an introduction to the decision making module of GPJax,\n",
    "which can be used to solve sequential decision making problems. Common examples of\n",
    "such problems include Bayesian optimisation (BO) and experimental design. For an\n",
    "in-depth introduction to Bayesian optimisation itself, be sure to checkout out our\n",
    "[Introduction to BO\n",
    "Notebook](https://docs.jaxgaussianprocesses.com/examples/bayesian_optimisation/).\n",
    "\n",
    "We'll be using BO as a case study to demonstrate how one may use the decision making\n",
    "module to solve sequential decision making problems. The goal of the decision making\n",
    "module is to provide a set of tools that can easily be used to solve a wide range of\n",
    "sequential decision making problems. The module is designed to be modular, and so it is\n",
    "easy to swap out different components of the decision making pipeline. Whilst it\n",
    "provides the functionality for quickly implementing a typical deicision making loop out\n",
    "of the box, we also hope that it will provide sufficient flexibility to allow users to\n",
    "define their own, more exotic, decision making pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2dc278",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Enable Float64 for more stable matrix inversions.\n",
    "from jax import config\n",
    "\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import optax as ox\n",
    "\n",
    "import gpjax as gpx\n",
    "from gpjax.decision_making.utility_functions import (\n",
    "    ThompsonSampling,\n",
    ")\n",
    "from gpjax.decision_making.utility_maximizer import (\n",
    "    ContinuousSinglePointUtilityMaximizer,\n",
    ")\n",
    "from gpjax.decision_making.decision_maker import UtilityDrivenDecisionMaker\n",
    "from gpjax.decision_making.utils import (\n",
    "    OBJECTIVE,\n",
    "    build_function_evaluator,\n",
    ")\n",
    "from gpjax.decision_making.posterior_handler import PosteriorHandler\n",
    "from gpjax.decision_making.search_space import ContinuousSearchSpace\n",
    "from gpjax.typing import (\n",
    "    Array,\n",
    "    Float,\n",
    ")\n",
    "\n",
    "key = jr.PRNGKey(42)\n",
    "plt.style.use(\n",
    "    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n",
    ")\n",
    "cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4d67a3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## The Black-Box Objective Function\n",
    "\n",
    "We'll be using the same problem as in the [Introduction to BO\n",
    "Notebook](https://docs.jaxgaussianprocesses.com/examples/bayesian_optimisation/), but\n",
    "rather than focussing on the mechanics of BO we'll be looking at how one may use the\n",
    "abstractions provided by the decision making module to implement the BO loop.\n",
    "\n",
    "In BO, and sequential decision making in general, we will often have a black-box\n",
    "function of interest which we can evaluate. In this notebook we'll be using the\n",
    "Forrester function as our objective to minimise:\n",
    "\n",
    "$$f(x) = (6x - 2)^2\\sin(12x-4)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411e409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forrester(x: Float[Array, \"N 1\"]) -> Float[Array, \"N 1\"]:\n",
    "    return (6 * x - 2) ** 2 * jnp.sin(12 * x - 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fecca7a",
   "metadata": {},
   "source": [
    "Within the decision making loop, we'll be querying the black-box objective function many\n",
    "times, and will often use the observed values to fit some probabilistic model. Thereore,\n",
    "it would be useful to have some method to which we can pass a set of points which we\n",
    "wish to query the black-box function at, and which will return a GPJax `Dataset` object\n",
    "containing the observations. We can use the `build_function_evaluator` function provided\n",
    "in `decision_making.utils` to do this. This function takes as input a dictionary of\n",
    "labelled black-box functions, and will return a function evaluator, which can be called\n",
    "with a set of points to evaluate the black-box functions at. The function evaluator will\n",
    "return a dictionary of labelled `Dataset` objects containing the observations. Note that\n",
    "in our case we only have one black-box function of interest, but in general we may have\n",
    "multiple different black-box functions, such as if we also have constraint functions.\n",
    "The use of the labels inside the dictionary returned by the function evaluator enables\n",
    "us to easily distinguish between these different observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fecb90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_evaluator = build_function_evaluator({OBJECTIVE: forrester})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fd236c",
   "metadata": {},
   "source": [
    "## The Search Space\n",
    "\n",
    "Having defined a method for evaluating the black-box function, we now need to define the\n",
    "search space over which we wish to optimise. In this case we'll be optimising over the\n",
    "interval $[0, 1]$. We can use the `ContinuousSearchSpace` class provided in\n",
    "`decision_making.search_space` to define this search space, as seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1831b21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bounds = jnp.array([0.0])\n",
    "upper_bounds = jnp.array([1.0])\n",
    "search_space = ContinuousSearchSpace(\n",
    "    lower_bounds=lower_bounds, upper_bounds=upper_bounds\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5705a1",
   "metadata": {},
   "source": [
    "The `ContinuousSearchSpace` class defines a `sample` method, which can be used to\n",
    "sample points from the search space using a space-filling design, in this case using the\n",
    "[Halton sequence](https://en.wikipedia.org/wiki/Halton_sequence). This will be useful at\n",
    "many points throughout the decision making loop, but for now let's use it to create an\n",
    "initial set of points which we can use to fit our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5105ebe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_x = search_space.sample(5, key)\n",
    "initial_datasets = function_evaluator(initial_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14523e9d",
   "metadata": {},
   "source": [
    "## The Surrogate Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026b8cf1",
   "metadata": {},
   "source": [
    "Many sequential decision making algorithms are described as being *model-based*. With\n",
    "these algorithms, we use a probabilistic model, or multiple models, to drive the\n",
    "decision making process. In ordinary BO, a probabilistic model is used to model the\n",
    "objective function, and it is updated based on observations from the black-box objective\n",
    "function. These models are often referred to as *surrogate models*, and are used to\n",
    "approximate the functions of interest. We'll be using the Gaussian process functionality\n",
    "provided by GPJax to define our surrogate models, with some wrappers provided by the\n",
    "`decision_making` module to make it easier to use these models within the decision\n",
    "making loop. We can proceed as usual when defining our priors, choosing a suitable\n",
    "mean function and kernel for the job at hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9c9ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = gpx.mean_functions.Zero()\n",
    "kernel = gpx.kernels.Matern52()\n",
    "prior = gpx.gps.Prior(mean_function=mean, kernel=kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae022cad",
   "metadata": {},
   "source": [
    "One difference from GPJax is the way in which we define our likelihood. In GPJax, we\n",
    "construct our GP posteriors by defining a `likelihood` object and then multiplying it\n",
    "with our prior to get the posterior, `posterior = likelihood * prior`. However, the\n",
    "`AbstractLikelihood` objects takes `num_datapoints` as one of its arguments, and this is\n",
    "going to be changing in the case of BO, and decision making in general, as we keep\n",
    "updating our models having observed new data! In order to deal with this we'll define a\n",
    "`likelihood_builder`, which takes as an argument the number of datapoints used to\n",
    "condition our prior on, and returns a `likelihood` object. Below we use this to\n",
    "construct a `likelihood_builder` which will return a `Gaussian` likelihood, initialised\n",
    "with the correct number of datapoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a9c9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood_builder = lambda n: gpx.likelihoods.Gaussian(\n",
    "    num_datapoints=n, obs_stddev=jnp.array(1e-3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805196de",
   "metadata": {},
   "source": [
    "Now we have all the components required for constructing our GP posterior. Since we'll\n",
    "be updating the posterior throughout the decision making loop as we observe more data,\n",
    "it would be useful to have an object which can handle all this logic for us.\n",
    "Fortunately, the `decision_making` module provides the `PosteriorHandler` class to do\n",
    "this for us. This class takes as input a `prior` and `likeligood_builder`, which we have\n",
    "defined above. We tend to also optimise the hyperparameters of the GP prior when\n",
    "\"fitting\" our GP, as demonstrated in the [Regression\n",
    "notebook](https://docs.jaxgaussianprocesses.com/examples/regression/). This will be\n",
    "using the GPJax `fit` method under the hood, which requires an `optimization_objective`,\n",
    "`optimizer` and `num_optimization_iters`. Therefore, we also pass these to the\n",
    "`PosteriorHandler` as demonstrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5f044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_handler = PosteriorHandler(\n",
    "    prior,\n",
    "    likelihood_builder=likelihood_builder,\n",
    "    optimization_objective=gpx.objectives.ConjugateMLL(negative=True),\n",
    "    optimizer=ox.adam(learning_rate=0.01),\n",
    "    num_optimization_iters=1000,\n",
    ")\n",
    "posterior_handlers = {OBJECTIVE: posterior_handler}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2557ca7",
   "metadata": {},
   "source": [
    "Note that we also create a labelled dictionary of `posterior_handlers`. This is a\n",
    "recurring theme with the decision making logic; we can have dictionaries containing\n",
    "datasets, posteriors and black box functions, and use labels to identify corresponding\n",
    "objects the dictionaries. For instance, here we have an \"OBJECTIVE\" posterior handler\n",
    "which is updated using the data in the \"OBJECTIVE\" dataset, which is in turn generated by the \"OBJECTIVE\" black-box function.\n",
    "\n",
    "Now, as the decision making loop progresses, we can use the `update_posterior` method of\n",
    "the `PosteriorHandler` to update our posterior as we observe more data. Note that we use\n",
    "the term *posterior* to refer to our GP posterior surrogate models in order to be\n",
    "consistent with the syntax used by GPJax. However, these GP posteriors are more widely\n",
    "referred to as *models* in the model-based decision making literature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef40fd30",
   "metadata": {},
   "source": [
    "## The Utility Function\n",
    "\n",
    "Now all that remains for us to define is the utiliy function, and a way of maximising\n",
    "it. Within the utility-driven decision making framework, we define a utility function,\n",
    "often using our GP surrogates, which characterises the *utility*, or *usefulness*, of\n",
    "querying the black-box function at any point within the domain of interest. We can then\n",
    "*maximise* this function to decide which point to query next. In this case we'll be\n",
    "using Thompson sampling as a utility function for determining where to query next. With\n",
    "this function we simply draw a sample from the GP posterior, and choose the minimizer\n",
    "of the sample as the point to query next. In the `decision_making` framework we create\n",
    "`UtilityFunctionBuilder` objects. Currently, we only support\n",
    "`SinglePointUtilityFunction`s, which are utility functions which characterise the\n",
    "utility of querying a single point. Thompson sampling is somewhat of a special case, as\n",
    "we can draw $B$ independent samples from the GP posterior and optimise each of these\n",
    "samples in order to obtain a *batch* of points to query next. We'll see an example of\n",
    "this later on.\n",
    "\n",
    "Within the `ThompsonSampling` utility function builder class we implement the\n",
    "`build_utility_function` method, which takes as input a dictionary containing lablled GP\n",
    "posteriors, as well as the corresponding datasets for these posteriors, and draws an\n",
    "approximate sample from the GP posterior which is a surrogate for the objective\n",
    "function. We instantiate our utility function builder below, specifying the number of\n",
    "Random Fourier features to use when constructing the approximate samples from the GP posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba11e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "utility_function_builder = ThompsonSampling(num_features=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d097a85",
   "metadata": {},
   "source": [
    "We also need a method for maximising the utility function. Since `ThompsonSampling` is\n",
    "classed as a `SinglePointUtilityFunction`, we can use the\n",
    "`ContinuousSinglePointUtilityMaximizer` to maximise it. This requires the user to\n",
    "specify `num_initial_samples` and `num_restarts` when instantiating it. This first\n",
    "queries the utility function at `num_initial_samples` points, and then uses the best of\n",
    "these points as a starting point for L-BFGS-B, a gradient-based optimiser, to further\n",
    "refine. This is repeated `num_restarts` times, each time sampling a different initial set\n",
    "of `num_initial_samples` and the best point found is returned. We'll instantiate our\n",
    "maximiser below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25c5c57",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "acquisition_maximizer = ContinuousSinglePointUtilityMaximizer(\n",
    "    num_initial_samples=100, num_restarts=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fdf1dc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Putting it All Together with the Decision Maker\n",
    "\n",
    "We now have all the ingredients ready for our Bayesian optimisation loop, so let's put\n",
    "all the logic together using the `UtilityDrivenDecisionMaker` class provided by the\n",
    "`decision_making` module. This class has 3 core methods:\n",
    "1. `ask` - This method is used to decide which point(s) to query next.\n",
    "2. `tell` - This method is used to tell the decision maker the results from querying the\n",
    "   black-box function at the points returned by `ask`, and will often update GP\n",
    "   posteriors in light of this data.\n",
    "3. `run` - This is used to run the decision making loop for a specified number of\n",
    "   iterations, alternating between `ask` and `tell`.\n",
    "\n",
    "For many decision making problems, the logic provided in the\n",
    "`UtilityDrivenDecisionMaker` will be sufficient, and is a convenient way of gluing the\n",
    "various bits of machinery involved in sequential decision making together. However, for\n",
    "more exotic decision making loops, it is easy for the user to define their own decision\n",
    "maker class by inheriting from the `AbstractDecisionMaker` class and defining their own\n",
    "`ask`, `tell` and `run` methods.\n",
    "\n",
    "However, we do also provide the user with some additional flexibility when using the\n",
    "`UtilityDrivenDecisionMaker` class. Often we may wish to perform certain actions after\n",
    "the `ask` step and the `tell` step, such as plotting the acquisition function and the\n",
    "point chosen to be queried for debugging purposes. We can do this by passing a list of\n",
    "functions to be called at each of these points as the `post_ask` and `post_tell`\n",
    "attributes of the `UtilityDrivenDecisionMaker`. Both sets of functions are called with\n",
    "the `UtilityDrivenDecisionMaker` as an argument, and so have access to all the\n",
    "attributes of the decision maker. The `post_ask` functions are additionally passed the\n",
    "most recently queried points too. We'll use this functionality to plot the acquisition\n",
    "function and the point chosen to be queried at each step of the decision making loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a068efaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bo_iteration(\n",
    "    dm: UtilityDrivenDecisionMaker, last_queried_points: Float[Array, \"B D\"]\n",
    "):\n",
    "    posterior = dm.posteriors[OBJECTIVE]\n",
    "    dataset = dm.datasets[OBJECTIVE]\n",
    "    plt_x = jnp.linspace(0, 1, 1000).reshape(-1, 1)\n",
    "    forrester_y = forrester(plt_x.squeeze(axis=-1))\n",
    "    utility_fn = dm.current_utility_functions[0]\n",
    "    sample_y = -utility_fn(plt_x)\n",
    "\n",
    "    latent_dist = posterior.predict(plt_x, train_data=dataset)\n",
    "    predictive_dist = posterior.likelihood(latent_dist)\n",
    "\n",
    "    predictive_mean = predictive_dist.mean()\n",
    "    predictive_std = predictive_dist.stddev()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(plt_x.squeeze(), predictive_mean, label=\"Predictive Mean\", color=cols[1])\n",
    "    ax.fill_between(\n",
    "        plt_x.squeeze(),\n",
    "        predictive_mean - 2 * predictive_std,\n",
    "        predictive_mean + 2 * predictive_std,\n",
    "        alpha=0.2,\n",
    "        label=\"Two sigma\",\n",
    "        color=cols[1],\n",
    "    )\n",
    "    ax.plot(\n",
    "        plt_x.squeeze(),\n",
    "        predictive_mean - 2 * predictive_std,\n",
    "        linestyle=\"--\",\n",
    "        linewidth=1,\n",
    "        color=cols[1],\n",
    "    )\n",
    "    ax.plot(\n",
    "        plt_x.squeeze(),\n",
    "        predictive_mean + 2 * predictive_std,\n",
    "        linestyle=\"--\",\n",
    "        linewidth=1,\n",
    "        color=cols[1],\n",
    "    )\n",
    "    ax.plot(plt_x.squeeze(), sample_y, label=\"Posterior Sample\")\n",
    "    ax.plot(\n",
    "        plt_x.squeeze(),\n",
    "        forrester_y,\n",
    "        label=\"Forrester Function\",\n",
    "        color=cols[0],\n",
    "        linestyle=\"--\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "    ax.axvline(x=0.757, linestyle=\":\", color=cols[3], label=\"True Optimum\")\n",
    "    ax.scatter(dataset.X, dataset.y, label=\"Observations\", color=cols[2], zorder=2)\n",
    "    ax.scatter(\n",
    "        last_queried_points[0],\n",
    "        -utility_fn(last_queried_points[0][None, ...]),\n",
    "        label=\"Posterior Sample Optimum\",\n",
    "        marker=\"*\",\n",
    "        color=cols[3],\n",
    "        zorder=3,\n",
    "    )\n",
    "    ax.legend(loc=\"center left\", bbox_to_anchor=(0.950, 0.5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd00f1b",
   "metadata": {},
   "source": [
    "Now let's put it all together and run our decision making loop for 6 iterations, with a\n",
    "batch size of 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec03aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = UtilityDrivenDecisionMaker(\n",
    "    search_space=search_space,\n",
    "    posterior_handlers=posterior_handlers,\n",
    "    datasets=initial_datasets,\n",
    "    utility_function_builder=utility_function_builder,\n",
    "    utility_maximizer=acquisition_maximizer,\n",
    "    batch_size=1,\n",
    "    key=key,\n",
    "    post_ask=[plot_bo_iteration],\n",
    "    post_tell=[],\n",
    ")\n",
    "\n",
    "results = dm.run(\n",
    "    6,\n",
    "    black_box_function_evaluator=function_evaluator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c490bdb",
   "metadata": {},
   "source": [
    "We can see that our `DecisionMaker` is successfully able to find the minimimizer of the\n",
    "black box function!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a12f9bd",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a4b7db",
   "metadata": {},
   "source": [
    "In this notebook we have provided an introduction to the new `decision_making` module of\n",
    "GPJax. We have demonstrated how one may use the abstractions provided by this module to\n",
    "implement a Bayesian optimisation loop, and have also highlighted some of the\n",
    "flexibility provided by the module. We hope that this module will provide a useful\n",
    "framework for solving a wide range of sequential decision making problems, and that it\n",
    "will be easy for users to extend the functionality provided by the module to suit their\n",
    "needs!\n",
    "\n",
    "We should note that the `decision_making` module is still in its early stages, and so\n",
    "whilst we hope to avoid making breaking changes to it, they may occur as the module\n",
    "evolves and more advanced functionality is implemented. If people have any feedback or\n",
    "features they would like to implement/see implemented, feel free to open an issue on the\n",
    "[GPJax GitHub page](https://github.com/JaxGaussianProcesses/GPJax/issues).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde1b078",
   "metadata": {},
   "source": [
    "## System Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f09b718",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -n -u -v -iv -w -a 'Thomas Christie'"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
