{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56aa1c29",
   "metadata": {},
   "source": [
    "# Likelihood guide\n",
    "\n",
    "In this notebook, we will walk users through the process of creating a new likelihood\n",
    "in GPJax.\n",
    "\n",
    "## Background\n",
    "\n",
    "In this section we'll provide a short introduction to likelihoods and why they are\n",
    "important. For users who are already familiar with likelihoods, feel free to skip to\n",
    "the next section, and for users who would like more information than is provided\n",
    "here, please see our [introduction to Gaussian processes notebook](intro_to_gps.py).\n",
    "\n",
    "### What is a likelihood?\n",
    "\n",
    "We adopt the notation of our\n",
    "[introduction to Gaussian processes notebook](intro_to_gps.py) where we have a\n",
    "Gaussian process (GP) $f(\\cdot)\\sim\\mathcal{GP}(m(\\cdot), k(\\cdot, \\cdot))$ and a\n",
    "dataset $\\mathbf{y} = \\{y_n\\}_{n=1}^N$ observed at corresponding inputs\n",
    "$\\mathbf{x} = \\{x_n\\}_{n=1}^N$. The evaluation of $f$ at $\\mathbf{x}$ is denoted by\n",
    "$\\mathbf{f} = \\{f(x_n)\\}_{n=1}^N$. The _likelihood function_ of the GP is then given\n",
    "by\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\label{eq:likelihood_fn}\n",
    "    p(\\mathbf{y}\\mid \\mathbf{f}) = \\prod_{n=1}^N p(y_n\\mid f(x_n))\\,.\n",
    "\\end{align}\n",
    "$$\n",
    "Conceptually, this conditional distribution describes the probability of the observed\n",
    "data, conditional on the latent function values.\n",
    "\n",
    "### Why is the likelihood important?\n",
    "\n",
    "Choosing the correct likelihood function when building a GP, or any Bayesian model\n",
    "for that matter, is crucial. The likelihood function encodes our assumptions about\n",
    "the data and the noise that we expect to observe. For example, if we are modelling\n",
    "air pollution, then we would not expect to observe negative values of pollution. In\n",
    "this case, we would choose a likelihood function that is only defined for positive\n",
    "values. Similarly, if our data is the proportion of people who voted for a particular\n",
    "political party, then we would expect to observe values between 0 and 1. In this\n",
    "case, we would choose a likelihood function that is only defined for values between\n",
    "0 and 1.\n",
    "\n",
    "## Likelihoods in GPJax\n",
    "\n",
    "In GPJax, all likelihoods are a subclass of the `AbstractLikelihood` class. This base\n",
    "abstract class contains the three core methods that all likelihoods must implement:\n",
    "`predict`, `link_function`, and `expected_log_likelihood`. We will discuss each of\n",
    "these methods in the forthcoming sections, but first, we will show how to instantiate\n",
    "a likelihood object. To do this, we'll need a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d987f5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Float64 for more stable matrix inversions.\n",
    "from jax import config\n",
    "\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "import gpjax as gpx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_probability.substrates.jax as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "plt.style.use(\n",
    "    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n",
    ")\n",
    "cols = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "key = jr.PRNGKey(123)\n",
    "\n",
    "\n",
    "n = 50\n",
    "x = jnp.sort(jr.uniform(key=key, shape=(n, 1), minval=-3.0, maxval=3.0), axis=0)\n",
    "xtest = jnp.linspace(-3, 3, 100)[:, None]\n",
    "f = lambda x: jnp.sin(x)\n",
    "y = f(x) + 0.1 * jr.normal(key, shape=x.shape)\n",
    "D = gpx.Dataset(x, y)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y, \"o\", label=\"Observations\")\n",
    "ax.plot(x, f(x), label=\"Latent function\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8217332",
   "metadata": {},
   "source": [
    "In this example, our observations have support $[-3, 3]$ and are generated from a\n",
    "sinusoidal function with Gaussian noise. As such, our response values $\\mathbf{y}$\n",
    "range between $-1$ and $1$, subject to Gaussian noise. Due to this, a Gaussian\n",
    "likelihood is appropriate for this dataset as it allows for negative values.\n",
    "\n",
    "As we see in \\eqref{eq:likelihood_fn}, the likelihood function factorises over the\n",
    "$n$ observations. As such, we must provide this information to GPJax when\n",
    "instantiating a likelihood object. We do this by specifying the `num_datapoints`\n",
    "argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a6c3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpx.likelihoods.Gaussian(num_datapoints=D.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800c07dc",
   "metadata": {},
   "source": [
    "### Likelihood parameters\n",
    "\n",
    "Some likelihoods, such as the Gaussian likelihood, contain parameters that we seek\n",
    "to infer. In the case of the Gaussian likelihood, we have a single parameter\n",
    "$\\sigma^2$ that determines the observation noise. In GPJax, we can specify the value\n",
    "of $\\sigma$ when instantiating the likelihood object. If we do not specify a\n",
    "value, then the likelihood will be initialised with a default value. In the case of\n",
    "the Gaussian likelihood, the default value is $1.0$. If we instead wanted to\n",
    "initialise the likelihood standard deviation with a value of $0.5$, then we would do\n",
    "this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8495a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpx.likelihoods.Gaussian(num_datapoints=D.n, obs_stddev=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8287a5a2",
   "metadata": {},
   "source": [
    "To control other properties of the observation noise such as trainability and value\n",
    "constraints, see our [PyTree guide](pytrees.md).\n",
    "\n",
    "### Prediction\n",
    "\n",
    "The `predict` method of a likelihood object transforms the latent distribution of\n",
    "the Gaussian process. In the case of a Gaussian likelihood, this simply applies the\n",
    "observational noise value to the diagonal values of the covariance matrix. For other\n",
    "likelihoods, this may be a more complex transformation. For example, the Bernoulli\n",
    "likelihood transforms the latent distribution of the Gaussian process into a\n",
    "distribution over binary values.\n",
    "\n",
    "We visualise this below for the Gaussian likelihood function. In blue we can see\n",
    "samples of $\\mathbf{f}^{\\star}$, whilst in red we see samples of\n",
    "$\\mathbf{y}^{\\star}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e369d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = gpx.kernels.Matern32()\n",
    "meanf = gpx.mean_functions.Zero()\n",
    "prior = gpx.gps.Prior(kernel=kernel, mean_function=meanf)\n",
    "\n",
    "likelihood = gpx.likelihoods.Gaussian(num_datapoints=D.n, obs_stddev=0.1)\n",
    "\n",
    "posterior = prior * likelihood\n",
    "\n",
    "latent_dist = posterior.predict(xtest, D)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=3, nrows=1, figsize=(9, 2))\n",
    "key, subkey = jr.split(key)\n",
    "\n",
    "for ax in axes.ravel():\n",
    "    subkey, _ = jr.split(subkey)\n",
    "    ax.plot(\n",
    "        latent_dist.sample(sample_shape=(1,), seed=subkey).T,\n",
    "        lw=1,\n",
    "        color=cols[0],\n",
    "        label=\"Latent samples\",\n",
    "    )\n",
    "    ax.plot(\n",
    "        likelihood.predict(latent_dist).sample(sample_shape=(1,), seed=subkey).T,\n",
    "        \"o\",\n",
    "        markersize=5,\n",
    "        alpha=0.3,\n",
    "        color=cols[1],\n",
    "        label=\"Predictive samples\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566a85d2",
   "metadata": {},
   "source": [
    "Similarly, for a Bernoulli likelihood function, the samples of $y$ would be binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdf751f",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = gpx.likelihoods.Bernoulli(num_datapoints=D.n)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(ncols=3, nrows=1, figsize=(9, 2))\n",
    "key, subkey = jr.split(key)\n",
    "\n",
    "for ax in axes.ravel():\n",
    "    subkey, _ = jr.split(subkey)\n",
    "    ax.plot(\n",
    "        latent_dist.sample(sample_shape=(1,), seed=subkey).T,\n",
    "        lw=1,\n",
    "        color=cols[0],\n",
    "        label=\"Latent samples\",\n",
    "    )\n",
    "    ax.plot(\n",
    "        likelihood.predict(latent_dist).sample(sample_shape=(1,), seed=subkey).T,\n",
    "        \"o\",\n",
    "        markersize=3,\n",
    "        alpha=0.5,\n",
    "        color=cols[1],\n",
    "        label=\"Predictive samples\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a974d298",
   "metadata": {},
   "source": [
    "### Link functions\n",
    "\n",
    "In the above figure, we can see the latent samples being constrained to be either 0 or\n",
    "1 when a Bernoulli likelihood is specified. This is achieved by the\n",
    "`inverse link_function` $\\eta(\\cdot)$ of the likelihood. The link function is a\n",
    "deterministic function that maps the latent distribution of the Gaussian process to\n",
    "the support of the likelihood function. For example, the link function of the\n",
    "Bernoulli likelihood that is used in GPJax is the inverse probit function\n",
    "$$\n",
    "\\eta(x) = 0.5\\left(1 + \\Phi\\left(\\frac{x}{\\sqrt{2}}\\right) * (1-2)\\right)\\,,\n",
    "$$\n",
    "where $\\Phi$ is the cumulative distribution function of the standard normal\n",
    "distribution.\n",
    "\n",
    "A table of commonly used link functions and their corresponding likelihood can be\n",
    "found [here](https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function).\n",
    "\n",
    "### Expected log likelihood\n",
    "\n",
    "The final method that is associated with a likelihood function in GPJax is the\n",
    "expected log-likelihood. This term is evaluated in the\n",
    "[stochastic variational Gaussian process](uncollaped_vi.py) in the ELBO term. For a\n",
    "variational approximation $q(f)= \\mathcal{N}(f\\mid m, S)$, the ELBO can be written as\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\label{eq:elbo}\n",
    "    \\mathcal{L}(q) = \\mathbb{E}_{f\\sim q(f)}\\left[ p(\\mathbf{y}\\mid f)\\right] - \\mathrm{KL}\\left(q(f)\\mid\\mid p(f)\\right)\\,.\n",
    "\\end{align}\n",
    "$$\n",
    "As both $q(f)$ and $p(f)$ are Gaussian distributions, the Kullback-Leibler term can\n",
    "be analytically computed. However, the expectation term is not always so easy to\n",
    "compute. Fortunately, the bound in \\eqref{eq:elbo} can be decomposed as a sum of the\n",
    "datapoints\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\label{eq:elbo_decomp}\n",
    "    \\mathcal{L}(q) = \\sum_{n=1}^N \\mathbb{E}_{f\\sim q(f)}\\left[ p(y_n\\mid f)\\right] - \\mathrm{KL}\\left(q(f)\\mid\\mid p(f)\\right)\\,.\n",
    "\\end{align}\n",
    "$$\n",
    "This simplifies computation of the expectation as it is now a series of $N$\n",
    "1-dimensional integrals. As such, GPJax by default uses quadrature to compute these\n",
    "integrals. However, for some likelihoods, such as the Gaussian likelihood, the\n",
    "expectation can be computed analytically. In these cases, we can supply an object\n",
    "that inherits from `AbstractIntegrator` to the likelihood upon instantiation. To see\n",
    "this, let us consider a Gaussian likelihood where we'll first define a variational\n",
    "approximation to the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640d8695",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = jnp.linspace(-3.0, 3.0, 10).reshape(-1, 1)\n",
    "q = gpx.variational_families.VariationalGaussian(posterior=posterior, inducing_inputs=z)\n",
    "\n",
    "\n",
    "def q_moments(x):\n",
    "    qx = q(x)\n",
    "    return qx.mean(), qx.variance()\n",
    "\n",
    "\n",
    "mean, variance = jax.vmap(q_moments)(x[:, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53ebd18",
   "metadata": {},
   "source": [
    "Now that we have the variational mean and variational (co)variance, we can compute\n",
    "the expected log-likelihood using the `expected_log_likelihood` method of the\n",
    "likelihood object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7901e689",
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.sum(likelihood.expected_log_likelihood(y=y, mean=mean, variance=variance))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c1fcf5",
   "metadata": {},
   "source": [
    "However, had we wanted to do this using quadrature, then we would have done the\n",
    "following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132be71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lquad = gpx.likelihoods.Gaussian(\n",
    "    num_datapoints=D.n,\n",
    "    obs_stddev=jnp.array([0.1]),\n",
    "    integrator=gpx.integrators.GHQuadratureIntegrator(num_points=20),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75aa2708",
   "metadata": {},
   "source": [
    "However, this is not recommended for the Gaussian likelihood given that the\n",
    "expectation can be computed analytically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6375fb6",
   "metadata": {},
   "source": [
    "## System configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e3bd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -n -u -v -iv -w -a 'Thomas Pinder'"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
