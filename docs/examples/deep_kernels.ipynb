{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60474ebd",
   "metadata": {},
   "source": [
    "# Deep Kernel Learning\n",
    "\n",
    "In this notebook we demonstrate how GPJax can be used in conjunction with\n",
    "[Flax](https://flax.readthedocs.io/en/latest/) to build deep kernel Gaussian\n",
    "processes. Modelling data with discontinuities is a challenging task for regular\n",
    "Gaussian process models. However, as shown in\n",
    "<strong data-cite=\"wilson2016deep\"></strong>, transforming the inputs to our\n",
    "Gaussian process model's kernel through a neural network can offer a solution to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c3b265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Float64 for more stable matrix inversions.\n",
    "from jax import config\n",
    "\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "from dataclasses import (\n",
    "    dataclass,\n",
    "    field,\n",
    ")\n",
    "from typing import Any\n",
    "\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "from jaxtyping import (\n",
    "    Array,\n",
    "    Float,\n",
    "    install_import_hook,\n",
    ")\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import optax as ox\n",
    "from scipy.signal import sawtooth\n",
    "from gpjax.base import static_field\n",
    "\n",
    "with install_import_hook(\"gpjax\", \"beartype.beartype\"):\n",
    "    import gpjax as gpx\n",
    "    from gpjax.base import param_field\n",
    "    import gpjax.kernels as jk\n",
    "    from gpjax.kernels import DenseKernelComputation\n",
    "    from gpjax.kernels.base import AbstractKernel\n",
    "    from gpjax.kernels.computations import AbstractKernelComputation\n",
    "\n",
    "key = jr.PRNGKey(123)\n",
    "plt.style.use(\n",
    "    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n",
    ")\n",
    "cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4024a24f",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "As previously mentioned, deep kernels are particularly useful when the data has\n",
    "discontinuities. To highlight this, we will use a sawtooth function as our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a17baba",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 500\n",
    "noise = 0.2\n",
    "\n",
    "key, subkey = jr.split(key)\n",
    "x = jr.uniform(key=key, minval=-2.0, maxval=2.0, shape=(n,)).reshape(-1, 1)\n",
    "f = lambda x: jnp.asarray(sawtooth(2 * jnp.pi * x))\n",
    "signal = f(x)\n",
    "y = signal + jr.normal(subkey, shape=signal.shape) * noise\n",
    "\n",
    "D = gpx.Dataset(X=x, y=y)\n",
    "\n",
    "xtest = jnp.linspace(-2.0, 2.0, 500).reshape(-1, 1)\n",
    "ytest = f(xtest)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y, \"o\", label=\"Training data\", alpha=0.5)\n",
    "ax.plot(xtest, ytest, label=\"True function\")\n",
    "ax.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6c6f18",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Deep kernels\n",
    "\n",
    "### Details\n",
    "\n",
    "Instead of applying a kernel $k(\\cdot, \\cdot')$ directly on some data, we seek to\n",
    "apply a _feature map_ $\\phi(\\cdot)$ that projects the data to learn more meaningful\n",
    "representations beforehand. In deep kernel learning, $\\phi$ is a neural network\n",
    "whose parameters are learned jointly with the GP model's hyperparameters. The\n",
    "corresponding kernel is then computed by $k(\\phi(\\cdot), \\phi(\\cdot'))$. Here\n",
    "$k(\\cdot,\\cdot')$ is referred to as the _base kernel_.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "Although deep kernels are not currently supported natively in GPJax, defining one is\n",
    "straightforward as we now demonstrate. Inheriting from the base `AbstractKernel`\n",
    "in GPJax, we create the `DeepKernelFunction` object that allows the\n",
    "user to supply the neural network and base kernel of their choice. Kernel matrices\n",
    "are then computed using the regular `gram` and `cross_covariance` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61debe6d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DeepKernelFunction(AbstractKernel):\n",
    "    base_kernel: AbstractKernel = None\n",
    "    network: nn.Module = static_field(None)\n",
    "    dummy_x: jax.Array = static_field(None)\n",
    "    key: jr.PRNGKeyArray = static_field(jr.PRNGKey(123))\n",
    "    nn_params: Any = field(init=False, repr=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.base_kernel is None:\n",
    "            raise ValueError(\"base_kernel must be specified\")\n",
    "        if self.network is None:\n",
    "            raise ValueError(\"network must be specified\")\n",
    "        self.nn_params = flax.core.unfreeze(self.network.init(key, self.dummy_x))\n",
    "\n",
    "    def __call__(\n",
    "        self, x: Float[Array, \" D\"], y: Float[Array, \" D\"]\n",
    "    ) -> Float[Array, \"1\"]:\n",
    "        state = self.network.init(self.key, x)\n",
    "        xt = self.network.apply(state, x)\n",
    "        yt = self.network.apply(state, y)\n",
    "        return self.base_kernel(xt, yt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5833763",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Defining a network\n",
    "\n",
    "With a deep kernel object created, we proceed to define a neural network. Here we\n",
    "consider a small multi-layer perceptron with two linear hidden layers and ReLU\n",
    "activation functions between the layers. The first hidden layer contains 64 units,\n",
    "while the second layer contains 32 units. Finally, we'll make the output of our\n",
    "network a three units wide. The corresponding kernel that we define will then be of\n",
    "[ARD form](https://docs.jaxgaussianprocesses.com/examples/constructing_new_kernels/#active-dimensions)\n",
    "to allow for different lengthscales in each dimension of the feature space.\n",
    "Users may wish to design more intricate network structures for more complex tasks,\n",
    "which functionality is supported well in Haiku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361a0b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_space_dim = 3\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \"\"\"A simple MLP.\"\"\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(features=32)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=64)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=feature_space_dim)(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "forward_linear = Network()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e209736",
   "metadata": {},
   "source": [
    "## Defining a model\n",
    "\n",
    "Having characterised the feature extraction network, we move to define a Gaussian\n",
    "process parameterised by this deep kernel. We consider a third-order Mat√©rn base\n",
    "kernel and assume a Gaussian likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f851a9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "base_kernel = gpx.kernels.Matern52(\n",
    "    active_dims=list(range(feature_space_dim)),\n",
    "    lengthscale=jnp.ones((feature_space_dim,)),\n",
    ")\n",
    "kernel = DeepKernelFunction(\n",
    "    network=forward_linear, base_kernel=base_kernel, key=key, dummy_x=x\n",
    ")\n",
    "meanf = gpx.mean_functions.Zero()\n",
    "prior = gpx.gps.Prior(mean_function=meanf, kernel=kernel)\n",
    "likelihood = gpx.likelihoods.Gaussian(num_datapoints=D.n)\n",
    "posterior = prior * likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5af346",
   "metadata": {},
   "source": [
    "### Optimisation\n",
    "\n",
    "We train our model via maximum likelihood estimation of the marginal log-likelihood.\n",
    "The parameters of our neural network are learned jointly with the model's\n",
    "hyperparameter set.\n",
    "\n",
    "With the inclusion of a neural network, we take this opportunity to highlight the\n",
    "additional benefits gleaned from using\n",
    "[Optax](https://optax.readthedocs.io/en/latest/) for optimisation. In particular, we\n",
    "showcase the ability to use a learning rate scheduler that decays the optimiser's\n",
    "learning rate throughout the inference. We decrease the learning rate according to a\n",
    "half-cosine curve over 700 iterations, providing us with large step sizes early in\n",
    "the optimisation procedure before approaching more conservative values, ensuring we\n",
    "do not step too far. We also consider a linear warmup, where the learning rate is\n",
    "increased from 0 to 1 over 50 steps to get a reasonable initial learning rate value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfca0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule = ox.warmup_cosine_decay_schedule(\n",
    "    init_value=0.0,\n",
    "    peak_value=0.01,\n",
    "    warmup_steps=75,\n",
    "    decay_steps=700,\n",
    "    end_value=0.0,\n",
    ")\n",
    "\n",
    "optimiser = ox.chain(\n",
    "    ox.clip(1.0),\n",
    "    ox.adamw(learning_rate=schedule),\n",
    ")\n",
    "\n",
    "opt_posterior, history = gpx.fit(\n",
    "    model=posterior,\n",
    "    objective=jax.jit(gpx.objectives.ConjugateMLL(negative=True)),\n",
    "    train_data=D,\n",
    "    optim=optimiser,\n",
    "    num_iters=800,\n",
    "    key=key,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563dd718",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "With a set of learned parameters, the only remaining task is to predict the output\n",
    "of the model. We can do this by simply applying the model to a test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d2c4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dist = opt_posterior(xtest, train_data=D)\n",
    "predictive_dist = opt_posterior.likelihood(latent_dist)\n",
    "\n",
    "predictive_mean = predictive_dist.mean()\n",
    "predictive_std = predictive_dist.stddev()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y, \"o\", label=\"Observations\", color=cols[0])\n",
    "ax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1])\n",
    "ax.fill_between(\n",
    "    xtest.squeeze(),\n",
    "    predictive_mean - 2 * predictive_std,\n",
    "    predictive_mean + 2 * predictive_std,\n",
    "    alpha=0.2,\n",
    "    color=cols[1],\n",
    "    label=\"Two sigma\",\n",
    ")\n",
    "ax.plot(\n",
    "    xtest,\n",
    "    predictive_mean - 2 * predictive_std,\n",
    "    color=cols[1],\n",
    "    linestyle=\"--\",\n",
    "    linewidth=1,\n",
    ")\n",
    "ax.plot(\n",
    "    xtest,\n",
    "    predictive_mean + 2 * predictive_std,\n",
    "    color=cols[1],\n",
    "    linestyle=\"--\",\n",
    "    linewidth=1,\n",
    ")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd562e42",
   "metadata": {},
   "source": [
    "## System configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f782f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -n -u -v -iv -w -a 'Thomas Pinder'"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
