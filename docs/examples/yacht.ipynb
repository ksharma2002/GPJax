{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "178587e1",
   "metadata": {},
   "source": [
    "# UCI Data Benchmarking\n",
    "\n",
    "In this notebook, we will show how to apply GPJax on a benchmark UCI regression\n",
    "problem. These kind of tasks are often used in the research community to benchmark\n",
    "and assess new techniques against those already in the literature. Much of the code\n",
    "contained in this notebook can be adapted to applied problems concerning datasets\n",
    "other than the one presented here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85c7cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Float64 for more stable matrix inversions.\n",
    "from jax import config\n",
    "\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "from jax import jit\n",
    "import jax.random as jr\n",
    "from jaxtyping import install_import_hook\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "with install_import_hook(\"gpjax\", \"beartype.beartype\"):\n",
    "    import gpjax as gpx\n",
    "\n",
    "# Enable Float64 for more stable matrix inversions.\n",
    "key = jr.PRNGKey(123)\n",
    "plt.style.use(\n",
    "    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n",
    ")\n",
    "cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16722d99",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "We'll be using the\n",
    "[Yacht](https://archive.ics.uci.edu/ml/datasets/yacht+hydrodynamics) dataset from\n",
    "the UCI machine learning data repository. Each observation describes the\n",
    "hydrodynamic performance of a yacht through its resistance. The dataset contains 6\n",
    "covariates and a single positive, real valued response variable. There are 308\n",
    "observations in the dataset, so we can comfortably use a conjugate regression\n",
    "Gaussian process here (for more more details, checkout the\n",
    "[Regression notebook](https://docs.jaxgaussianprocesses.com/examples/regression/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655a0fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    yacht = pd.read_fwf(\"data/yacht_hydrodynamics.data\", header=None).values[:-1, :]\n",
    "except FileNotFoundError:\n",
    "    yacht = pd.read_fwf(\n",
    "        \"docs/examples/data/yacht_hydrodynamics.data\", header=None\n",
    "    ).values[:-1, :]\n",
    "\n",
    "X = yacht[:, :-1]\n",
    "y = yacht[:, -1].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97534a81",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "With a dataset loaded, we'll now preprocess it such that it is more amenable to\n",
    "modelling with a Gaussian process.\n",
    "\n",
    "### Data Partitioning\n",
    "\n",
    "We'll first partition our data into a _training_ and _testing_ split. We'll fit our\n",
    "Gaussian process to the training data and evaluate its performance on the test data.\n",
    "This allows us to investigate how effectively our Gaussian process generalises to\n",
    "out-of-sample datapoints and ensure that we are not overfitting. We'll hold 30% of\n",
    "our data back for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c531438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7878786",
   "metadata": {},
   "source": [
    "### Response Variable\n",
    "\n",
    "We'll now process our response variable $\\mathbf{y}$. As the below plots show, the\n",
    "data has a very long tail and is certainly not Gaussian. However, we would like to\n",
    "model a Gaussian response variable so that we can adopt a Gaussian likelihood\n",
    "function and leverage the model's conjugacy. To achieve this, we'll first log-scale\n",
    "the data, to bring the long right tail in closer to the data's mean. We'll then\n",
    "standardise the data such that is distributed according to a unit normal\n",
    "distribution. Both of these transformations are invertible through the log-normal\n",
    "expectation and variance formulae and the the inverse standardisation identity,\n",
    "should we ever need our model's predictions to be back on the scale of the\n",
    "original dataset.\n",
    "\n",
    "For transforming both the input and response variable, all transformations will be\n",
    "done with respect to the training data where relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256bd82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_ytr = np.log(ytr)\n",
    "log_yte = np.log(yte)\n",
    "\n",
    "y_scaler = StandardScaler().fit(log_ytr)\n",
    "scaled_ytr = y_scaler.transform(log_ytr)\n",
    "scaled_yte = y_scaler.transform(log_yte)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea87d30f",
   "metadata": {},
   "source": [
    "We can see the effect of these transformations in the below three panels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cc2e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(9, 2.5))\n",
    "ax[0].hist(ytr, bins=30, color=cols[1])\n",
    "ax[0].set_title(\"y\")\n",
    "ax[1].hist(log_ytr, bins=30, color=cols[1])\n",
    "ax[1].set_title(\"log(y)\")\n",
    "ax[2].hist(scaled_ytr, bins=30, color=cols[1])\n",
    "ax[2].set_title(\"scaled log(y)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f0e17c",
   "metadata": {},
   "source": [
    "### Input Variable\n",
    "\n",
    "We'll now transform our input variable $\\mathbf{X}$ to be distributed according to a\n",
    "unit Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf6e9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaler = StandardScaler().fit(Xtr)\n",
    "scaled_Xtr = x_scaler.transform(Xtr)\n",
    "scaled_Xte = x_scaler.transform(Xte)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ca471d",
   "metadata": {},
   "source": [
    "## Model fitting\n",
    "\n",
    "With data now loaded and preprocessed, we'll proceed to defining a Gaussian process\n",
    "model and optimising its parameters. This notebook purposefully does not go into\n",
    "great detail on this process, so please see notebooks such as the\n",
    "[Regression notebook](https://docs.jaxgaussianprocesses.com/examples/regression/)\n",
    "and\n",
    "[Classification notebook](https://docs.jaxgaussianprocesses.com/examples/classification)\n",
    "for further information.\n",
    "\n",
    "### Model specification\n",
    "\n",
    "We'll use a radial basis function kernel to parameterise the Gaussian process in this\n",
    "notebook. As we have 5 covariates, we'll assign each covariate its own lengthscale\n",
    "parameter. This form of kernel is commonly known as an automatic relevance\n",
    "determination (ARD) kernel.\n",
    "\n",
    "In practice, the exact form of kernel used should be selected such that it\n",
    "represents your understanding of the data. For example, if you were to model\n",
    "temperature; a process that we know to be periodic, then you would likely wish to\n",
    "select a periodic kernel. Having _Gaussian-ised_ our data somewhat, we'll also adopt\n",
    "a Gaussian likelihood function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fb6566",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train, n_covariates = scaled_Xtr.shape\n",
    "kernel = gpx.kernels.RBF(\n",
    "    active_dims=list(range(n_covariates)),\n",
    "    variance=np.var(scaled_ytr),\n",
    "    lengthscale=0.1 * np.ones((n_covariates,)),\n",
    ")\n",
    "meanf = gpx.mean_functions.Zero()\n",
    "prior = gpx.gps.Prior(mean_function=meanf, kernel=kernel)\n",
    "\n",
    "likelihood = gpx.likelihoods.Gaussian(num_datapoints=n_train)\n",
    "\n",
    "posterior = prior * likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fbe4d2",
   "metadata": {},
   "source": [
    "### Model Optimisation\n",
    "\n",
    "With a model now defined, we can proceed to optimise the hyperparameters of our\n",
    "model using Scipy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1655d37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = gpx.Dataset(X=scaled_Xtr, y=scaled_ytr)\n",
    "\n",
    "negative_mll = jit(gpx.objectives.ConjugateMLL(negative=True))\n",
    "\n",
    "opt_posterior, history = gpx.fit_scipy(\n",
    "    model=posterior,\n",
    "    objective=negative_mll,\n",
    "    train_data=training_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f627693",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "With an optimal set of parameters learned, we can make predictions on the set of\n",
    "data that we held back right at the start. We'll do this in the usual way by first\n",
    "computing the latent function's distribution before computing the predictive\n",
    "posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68f32ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dist = opt_posterior(scaled_Xte, training_data)\n",
    "predictive_dist = likelihood(latent_dist)\n",
    "\n",
    "predictive_mean = predictive_dist.mean()\n",
    "predictive_stddev = predictive_dist.stddev()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38c09b5",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We'll now show how the performance of our Gaussian process can be evaluated by\n",
    "numerically and visually.\n",
    "\n",
    "### Metrics\n",
    "\n",
    "To numerically assess the performance of our model, two commonly used metrics are\n",
    "root mean squared error (RMSE) and the R2 coefficient. RMSE is simply the square\n",
    "root of the squared difference between predictions and actuals. A value of 0 for\n",
    "this metric implies that our model has 0 generalisation error on the test set. R2\n",
    "measures the amount of variation within the data that is explained by the model.\n",
    "This can be useful when designing variance reduction methods such as control\n",
    "variates as it allows you to understand what proportion of the data's variance will\n",
    "be soaked up. A perfect model here would score 1 for R2 score, whereas predicting\n",
    "the data's mean would score 0 and models doing worse than simple mean predictions\n",
    "can score less than 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d5eeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = mean_squared_error(y_true=scaled_yte.squeeze(), y_pred=predictive_mean)\n",
    "r2 = r2_score(y_true=scaled_yte.squeeze(), y_pred=predictive_mean)\n",
    "print(f\"Results:\\n\\tRMSE: {rmse: .4f}\\n\\tR2: {r2: .2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167c83d9",
   "metadata": {},
   "source": [
    "Both of these metrics seem very promising, so, based off these, we can be quite\n",
    "happy that our first attempt at modelling the Yacht data is promising.\n",
    "\n",
    "### Diagnostic plots\n",
    "\n",
    "To accompany the above metrics, we can also produce residual plots to explore\n",
    "exactly where our model's shortcomings lie. If we define a residual as the true\n",
    "value minus the prediction, then we can produce three plots:\n",
    "\n",
    "1. Predictions vs. actuals.\n",
    "2. Predictions vs. residuals.\n",
    "3. Residual density.\n",
    "\n",
    "The first plot allows us to explore if our model struggles to predict well for\n",
    "larger or smaller values by observing where the model deviates more from the line\n",
    "$y=x$. In the second plot we can inspect whether or not there were outliers or\n",
    "structure within the errors of our model. A well-performing model would have\n",
    "predictions close to and symmetrically distributed either side of $y=0$. Such a\n",
    "plot can be useful for diagnosing heteroscedasticity. Finally, by plotting a\n",
    "histogram of our residuals we can observe whether or not there is any skew to\n",
    "our residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f05f6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = scaled_yte.squeeze() - predictive_mean\n",
    "\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(9, 2.5), tight_layout=True)\n",
    "\n",
    "ax[0].scatter(predictive_mean, scaled_yte.squeeze(), color=cols[1])\n",
    "ax[0].plot([0, 1], [0, 1], color=cols[0], transform=ax[0].transAxes)\n",
    "ax[0].set(xlabel=\"Predicted\", ylabel=\"Actual\", title=\"Predicted vs Actual\")\n",
    "\n",
    "ax[1].scatter(predictive_mean.squeeze(), residuals, color=cols[1])\n",
    "ax[1].plot([0, 1], [0.5, 0.5], color=cols[0], transform=ax[1].transAxes)\n",
    "ax[1].set_ylim([-1.0, 1.0])\n",
    "ax[1].set(xlabel=\"Predicted\", ylabel=\"Residuals\", title=\"Predicted vs Residuals\")\n",
    "\n",
    "ax[2].hist(np.asarray(residuals), bins=30, color=cols[1])\n",
    "ax[2].set_title(\"Residuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1846bcb",
   "metadata": {},
   "source": [
    "From this, we can see that our model is struggling to predict the smallest values\n",
    "of the Yacht's hydrodynamic and performs increasingly well as the Yacht's\n",
    "hydrodynamic performance increases. This is likely due to the original data's heavy\n",
    "right-skew, and successive modelling attempts may wish to introduce a\n",
    "heteroscedastic likelihood function that would enable more flexible modelling of\n",
    "the smaller response values.\n",
    "\n",
    "## System configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055301ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -n -u -v -iv -w -a 'Thomas Pinder'"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "custom_cell_magics": "kql"
  },
  "kernelspec": {
   "display_name": "gpjax",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
