{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1be933f",
   "metadata": {},
   "source": [
    "# Graph Kernels\n",
    "\n",
    "This notebook demonstrates how regression models can be constructed on the vertices\n",
    "of a graph using a Gaussian process with a Mat√©rn kernel presented in\n",
    "<strong data-cite=\"borovitskiy2021matern\"></strong>. For a general discussion of the\n",
    "kernels supported within GPJax, see the\n",
    "[kernels notebook](https://docs.jaxgaussianprocesses.com/examples/constructing_new_kernels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e34619b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Float64 for more stable matrix inversions.\n",
    "from jax import config\n",
    "\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "import random\n",
    "\n",
    "from jax import jit\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "from jaxtyping import install_import_hook\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import optax as ox\n",
    "\n",
    "with install_import_hook(\"gpjax\", \"beartype.beartype\"):\n",
    "    import gpjax as gpx\n",
    "\n",
    "key = jr.PRNGKey(123)\n",
    "plt.style.use(\n",
    "    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n",
    ")\n",
    "cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2e1130",
   "metadata": {},
   "source": [
    "## Graph construction\n",
    "\n",
    "Our graph $\\mathcal{G}=\\lbrace V, E \\rbrace$ comprises a set of vertices\n",
    "$V = \\lbrace v_1, v_2, \\ldots, v_n\\rbrace$ and edges\n",
    "$E=\\lbrace (v_i, v_j)\\in V \\ : \\ i \\neq j\\rbrace$. In particular, we will consider\n",
    "a [barbell graph](https://en.wikipedia.org/wiki/Barbell_graph) that is an undirected\n",
    "graph containing two clusters of vertices with a single shared edge between the\n",
    "two clusters.\n",
    "\n",
    "Contrary to the typical barbell graph, we'll randomly remove a subset of 30 edges\n",
    "within each of the two clusters. Given the 40 vertices within the graph, this results\n",
    "in 351 edges as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ee92a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_per_side = 20\n",
    "n_edges_to_remove = 30\n",
    "p = 0.8\n",
    "\n",
    "G = nx.barbell_graph(vertex_per_side, 0)\n",
    "\n",
    "random.seed(123)\n",
    "[G.remove_edge(*i) for i in random.sample(list(G.edges), n_edges_to_remove)]\n",
    "\n",
    "pos = nx.spring_layout(G, seed=123)  # positions for all nodes\n",
    "\n",
    "nx.draw(\n",
    "    G, pos, node_size=100, node_color=cols[1], edge_color=\"black\", with_labels=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62865d6",
   "metadata": {},
   "source": [
    "\n",
    "### Computing the graph Laplacian\n",
    "\n",
    "Graph kernels use the _Laplacian matrix_ $L$ to quantify the smoothness of a signal\n",
    "(or function) on a graph\n",
    "$$L=D-A,$$\n",
    "where $D$ is the diagonal _degree matrix_ containing each vertices' degree and $A$\n",
    "is the _adjacency matrix_ that has an $(i,j)^{\\text{th}}$ entry of 1 if $v_i, v_j$\n",
    "are connected and 0 otherwise. [Networkx](https://networkx.org) gives us an easy\n",
    "way to compute this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e005f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = nx.laplacian_matrix(G).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f782d7b",
   "metadata": {},
   "source": [
    "\n",
    "## Simulating a signal on the graph\n",
    "\n",
    "Our task is to construct a Gaussian process $f(\\cdot)$ that maps from the graph's\n",
    "vertex set $V$ onto the real line.\n",
    "To that end, we begin by simulating a signal on the graph's vertices that we will go\n",
    "on to try and predict.\n",
    "We use a single draw from a Gaussian process prior to draw our response values\n",
    "$\\boldsymbol{y}$ where we hardcode parameter values.\n",
    "The corresponding input value set for this model, denoted $\\boldsymbol{x}$, is the\n",
    "index set of the graph's vertices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e9105e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.arange(G.number_of_nodes()).reshape(-1, 1)\n",
    "\n",
    "true_kernel = gpx.kernels.GraphKernel(\n",
    "    laplacian=L,\n",
    "    lengthscale=2.3,\n",
    "    variance=3.2,\n",
    "    smoothness=6.1,\n",
    ")\n",
    "prior = gpx.gps.Prior(mean_function=gpx.mean_functions.Zero(), kernel=true_kernel)\n",
    "\n",
    "fx = prior(x)\n",
    "y = fx.sample(seed=key, sample_shape=(1,)).reshape(-1, 1)\n",
    "\n",
    "D = gpx.Dataset(X=x, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b531b71",
   "metadata": {},
   "source": [
    "\n",
    "We can visualise this signal in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbf0f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(G, pos, node_color=y, with_labels=False, alpha=0.5)\n",
    "\n",
    "vmin, vmax = y.min(), y.max()\n",
    "sm = plt.cm.ScalarMappable(\n",
    "    cmap=plt.cm.inferno, norm=plt.Normalize(vmin=vmin, vmax=vmax)\n",
    ")\n",
    "sm.set_array([])\n",
    "ax = plt.gca()\n",
    "cbar = plt.colorbar(sm, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5fa6d1",
   "metadata": {},
   "source": [
    "\n",
    "## Constructing a graph Gaussian process\n",
    "\n",
    "With our dataset created, we proceed to define our posterior Gaussian process and\n",
    "optimise the model's hyperparameters.\n",
    "Whilst our underlying space is the graph's vertex set and is therefore\n",
    "non-Euclidean, our likelihood is still Gaussian and the model is still conjugate.\n",
    "For this reason, we simply perform gradient descent on the GP's marginal\n",
    "log-likelihood term as in the\n",
    "[regression notebook](https://docs.jaxgaussianprocesses.com/examples/regression/).\n",
    "We do this using the BFGS optimiser provided in `scipy` via 'jaxopt'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b30c399",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = gpx.likelihoods.Gaussian(num_datapoints=D.n)\n",
    "kernel = gpx.kernels.GraphKernel(laplacian=L)\n",
    "prior = gpx.gps.Prior(mean_function=gpx.mean_functions.Zero(), kernel=kernel)\n",
    "posterior = prior * likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd048524",
   "metadata": {},
   "source": [
    "\n",
    "For researchers and the curious reader, GPJax provides the ability to print the\n",
    "bibtex citation for objects such as the graph kernel through the `cite()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f757b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gpx.cite(kernel))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87623937",
   "metadata": {},
   "source": [
    "\n",
    "With a posterior defined, we can now optimise the model's hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dd068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_posterior, training_history = gpx.fit_scipy(\n",
    "    model=posterior,\n",
    "    objective=gpx.objectives.ConjugateMLL(negative=True),\n",
    "    train_data=D,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebbba4a",
   "metadata": {},
   "source": [
    "\n",
    "## Making predictions\n",
    "\n",
    "Having optimised our hyperparameters, we can now make predictions on the graph.\n",
    "Though we haven't defined a training and testing dataset here, we'll simply query\n",
    "the predictive posterior for the full graph to compare the root-mean-squared error\n",
    "(RMSE) of the model for the initialised parameters vs the optimised set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbed189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_dist = likelihood(posterior(x, D))\n",
    "predictive_dist = opt_posterior.likelihood(opt_posterior(x, D))\n",
    "\n",
    "initial_mean = initial_dist.mean()\n",
    "learned_mean = predictive_dist.mean()\n",
    "\n",
    "rmse = lambda ytrue, ypred: jnp.sum(jnp.sqrt(jnp.square(ytrue - ypred)))\n",
    "\n",
    "initial_rmse = jnp.sum(jnp.sqrt(jnp.square(y.squeeze() - initial_mean)))\n",
    "learned_rmse = jnp.sum(jnp.sqrt(jnp.square(y.squeeze() - learned_mean)))\n",
    "print(\n",
    "    f\"RMSE with initial parameters: {initial_rmse: .2f}\\nRMSE with learned parameters:\"\n",
    "    f\" {learned_rmse: .2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dfd50a",
   "metadata": {},
   "source": [
    "\n",
    "We can also plot the source of error in our model's predictions on the graph by the\n",
    "following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69be0310",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = jnp.abs(learned_mean - y.squeeze())\n",
    "\n",
    "nx.draw(G, pos, node_color=error, with_labels=False, alpha=0.5)\n",
    "\n",
    "vmin, vmax = error.min(), error.max()\n",
    "sm = plt.cm.ScalarMappable(\n",
    "    cmap=plt.cm.inferno, norm=plt.Normalize(vmin=vmin, vmax=vmax)\n",
    ")\n",
    "ax = plt.gca()\n",
    "cbar = plt.colorbar(sm, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b226e330",
   "metadata": {},
   "source": [
    "\n",
    "Reassuringly, our model seems to provide equally good predictions in each cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ae90f8",
   "metadata": {},
   "source": [
    "## System configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e566842",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -n -u -v -iv -w -a 'Thomas Pinder'"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
